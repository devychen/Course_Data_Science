{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf56e8b-0e42-4109-920c-4b06bf53be17",
   "metadata": {},
   "source": [
    "# a) \n",
    "Load the contents of the UTF-8 encoded file sq-sample.txt into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db79e024-e5cc-41dd-b88e-6bab28ea227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/ychen/WISE24/sq-sample.txt'\n",
    "with open('sq-sample.txt', 'r', encoding='utf-8') as file:\n",
    "    encoded_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2c6c9-eb8a-4410-a53a-8de3efd60765",
   "metadata": {},
   "source": [
    "# b) \n",
    "Tokenize the text by splitting it at whitespaces and linebreaks, \n",
    "turn all tokens to lowercase and remove punctuation symbols (,, ., etc.), \n",
    "store result in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a16640-4bc1-460d-ab19-a2cb06609fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regular expressions:\n",
    "\n",
    "import re\n",
    "\n",
    "# split at linebreaks and whitespaces\n",
    "\n",
    "tokens = re.split(r'\\s+|\\n+', encoded_text)\n",
    "\n",
    "# lowercase\n",
    "\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# remove punctuation (any non-alphanumeric character)\n",
    "\n",
    "tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "\n",
    "# Store the result in a list\n",
    "\n",
    "token_list = tokens\n",
    "\n",
    "# Print parts of the token list to check\n",
    "\n",
    "print(token_list[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12814c17-99d3-457e-b46f-3aa319a25854",
   "metadata": {},
   "source": [
    "# c) \n",
    "Write a function which converts a list of tokens into a Counter of token frequencies.  \n",
    "\n",
    "编写一个函数，该函数将标记列表转换为标记频率（token frequencies）的计数器（Counter）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fac165-373e-45e8-8074-05beae4f3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_tokens(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# check output\n",
    "token_counter = count_tokens(token_list)\n",
    "print(token_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6ab83-61ec-419c-ae54-ed0b5c1b5986",
   "metadata": {},
   "source": [
    "# d) \n",
    "Separate the tokenized text into ten chunks of equal size, run them through the function.\n",
    "\n",
    "将标记化的文本分成十个相等大小的块（chunks），然后将这些块分别传递给上面编写的函数，以获取每个块内的标记频率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aaa726-3972-46a1-90d9-fc95349e64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into 10 equal-sized chunks\n",
    "\n",
    "chunk_size = len(tokens) // 10\n",
    "token_chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "# Obtain token frequencies for each chunk\n",
    "\n",
    "chunk_frequencies = [token_frequency(chunk) for chunk in token_chunks]\n",
    "\n",
    "# I am not sure how to do this one. Got stuck from here. The following codes do not seem to be the right solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f85ab4-5b1e-49b7-ac20-f1be6f29572a",
   "metadata": {},
   "source": [
    "# e) \n",
    "Use defaultdict to create a map from tokens to a set of those chunk IDs where the token was among the top-625 tokens (i.e. roughly equivalent to A1 level).\n",
    "\n",
    "使用 defaultdict 来创建一个映射，将标记与那些在顶部 625 个标记之一的十个块ID集合相关联。这意味着你需要找出在每个块中出现最频繁的标记（大约等同于 A1 级别的标记，这是一种语言能力级别），然后将它们映射到对应的块ID集合中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4f7eb-970f-4263-8b68-71cf1207d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Combine token frequencies from all chunks\n",
    "combined_frequencies = Counter()\n",
    "for freq in chunk_frequencies:\n",
    "    combined_frequencies.update(freq)\n",
    "\n",
    "# Get the top 625 tokens (change this number if needed)\n",
    "top_tokens = [token for token, _ in combined_frequencies.most_common(625)]\n",
    "\n",
    "# Create a defaultdict to store token-to-chunk mapping\n",
    "token_to_chunks = defaultdict(set)\n",
    "\n",
    "# Iterate through chunks and identify which tokens are in the top 625\n",
    "for i, chunk_freq in enumerate(chunk_frequencies):\n",
    "    for token, frequency in chunk_freq.items():\n",
    "        if token in top_tokens:\n",
    "            token_to_chunks[token].add(i)\n",
    "\n",
    "# print the map\n",
    "for token, chunk_ids in token_to_chunks.items():\n",
    "    print(f\"Token: {token}, Chunk IDs: {chunk_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f278870-ce8f-418a-a7b3-dd0d15d740fa",
   "metadata": {},
   "source": [
    "# Based on the objects resulting from above, answer the following questions:\n",
    "# a) \n",
    "How many words were in the top-625 across all of the ten chunks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55255e-5639-4fee-9ea5-83ee1a348560",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words_in_top_625 = len(top_tokens)\n",
    "print(total_words_in_top_625)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929092c8-e2de-400c-ab57-c6a18b02b18b",
   "metadata": {},
   "source": [
    "# b) \n",
    "Create a two-dimensional array of top-625 overlaps for each pair of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa7450-50d1-4bf4-8d04-ceaa9fa7da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_overlaps = [[len(set(chunk1) & set(chunk2)) for chunk2 in chunk_frequencies] for chunk1 in chunk_frequencies]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ebeb1-805b-4ff2-bbf4-c7f810d8512b",
   "metadata": {},
   "source": [
    "# c) \n",
    "Is there a chunk which seems to be especially divergent from the rest of the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880803b1-0d70-47ca-8422-f8afdc1629a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_top_words_in_chunks = [len(set(chunk) & set(top_tokens)) for chunk in chunk_frequencies]\n",
    "\n",
    "# Find the index of the most divergent chunk\n",
    "most_divergent_chunk_index = total_top_words_in_chunks.index(min(total_top_words_in_chunks))\n",
    "print(\"The most divergent chunk is Chunk\", most_divergent_chunk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315a9fc-fc68-4788-a962-dce9a9947a3f",
   "metadata": {},
   "source": [
    "# d) \n",
    "Does this result tell you anything useful about frequency lists?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2507de44-eaa6-41ee-ad73-ef052fc8b6a1",
   "metadata": {},
   "source": [
    "The result shows that the top-625 words are consistent across most of the text, but Chunk 10 stands out as being fundamentally divergent from the others. I am not sure about its interpretation in linguistic data analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
