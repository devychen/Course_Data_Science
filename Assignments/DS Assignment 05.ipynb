{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf002a-592f-45dd-bbfe-b41bfbbbd6be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install SpaCy\n",
    "# !pip install spacy\n",
    "# download the model es_dep_news_trf.\n",
    "!python3 -m spacy download es_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6df86-bcf3-41b2-abe1-d63377124d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1 & Q2\n",
    "Install SpaCy and let it download the model es_dep_news_trf.\n",
    "Create a new Jupyter notebook and import spacy. \n",
    "Load the contents of the file azuela1920_los-de-abajo.txt into a single string, and run the model on it. \n",
    "Extract the sentence spans, and store their contents into a text file.\n",
    "'''\n",
    "\n",
    "# import spacy\n",
    "import spacy\n",
    "\n",
    "# Load the Spanish transformer-based model\n",
    "nlp = spacy.load(\"es_dep_news_trf\")\n",
    "\n",
    "# Load the contents of the file into a single string\n",
    "file_path = \"/Users/ychen/DS_Assign/azuela1920_los-de-abajo.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Run model on it\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract sentence spans\n",
    "sentence_spans = list(doc.sents)\n",
    "\n",
    "# Store sentence contents in a text file\n",
    "with open(\"sentence_spans.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for span in sentence_spans:\n",
    "        output_file.write(span.text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c840009-6674-4bff-b3d2-e5e18a803636",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3\n",
    "Repeat the process, but this time, replace all newline characters in the text by a space before running it through the pipeline. \n",
    "Store the sentence spans in a file again, and investigate the differences. \n",
    "What do you notice? Use machine translation in case you don’t understand enough. \n",
    "For the following tasks, we will stick to the version without newlines.\n",
    "'''\n",
    "\n",
    "# Read the contents of the file into a string and replace newlines with spaces before running it through the pipeline\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_modified = file.read().replace(\"\\n\", \" \")\n",
    "\n",
    "# Process the modified text with SpaCy\n",
    "doc_modified = nlp(text_modified)\n",
    "\n",
    "# Extract sentence spans again\n",
    "sentence_spans_modified = list(doc_modified.sents)\n",
    "\n",
    "# Store the modified new sentence contents in a new text file\n",
    "with open(\"sentence_spans_modified.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for span_modified in sentence_spans_modified:\n",
    "        output_file.write(span_modified.text + \"\\n\")\n",
    "'''\n",
    "Differences:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434780b0-f155-4667-b8b9-e998f9c3efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4\n",
    "Familiarise yourself with the different properties of the Token object, \n",
    "and extract all pairs of full verbs (UD tag VERB) and their subjects (relation nsubj) in lemmatised form, \n",
    "storing them for later processing. In my solution, the first pair is (’decir’,’parte’) (which is actually wrong).\n",
    "'''\n",
    "# Initialize a list to store verb-subject pairs\n",
    "verb_subject_pairs = []\n",
    "\n",
    "# Iterate over the tokens in the processed document\n",
    "for token in doc:\n",
    "    # Check if the token is a verb and has a subject\n",
    "    if token.pos_ == \"VERB\" and \"nsubj\" in [child.dep_ for child in token.children]:\n",
    "        # Get the lemmatized form of the verb and its subject\n",
    "        verb_lemma = token.lemma_\n",
    "        subject = [child.text for child in token.children if child.dep_ == \"nsubj\"]\n",
    "\n",
    "        # Append the verb-subject pair to the list\n",
    "        verb_subject_pairs.append((verb_lemma, subject))\n",
    "\n",
    "# Print the first few pairs for verification\n",
    "print(verb_subject_pairs[:5])\n",
    "\n",
    "# Store the verb-subject pairs in a file for later processing\n",
    "with open(\"verb_subject_pairs.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for pair in verb_subject_pairs:\n",
    "        output_file.write(f\"{pair[0]}, {', '.join(pair[1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196ca92-5c84-4be1-a065-a7a4289aef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5\n",
    "Extract the ten most common verbs occurring in your pairs, \n",
    "and the three most common subjects for the following verbs: \n",
    "gritar “to shout”, preguntar “to ask”, and responder “to answer”. \n",
    "Who shouts the most, who asks the most questions, and who appears to answer them?\n",
    "'''\n",
    "from collections import Counter\n",
    "\n",
    "# Extract all verbs from the verb-subject pairs\n",
    "all_verbs = [pair[0] for pair in verb_subject_pairs]\n",
    "\n",
    "# Get the ten most common verbs\n",
    "common_verbs = [verb for verb, _ in Counter(all_verbs).most_common(10)]\n",
    "\n",
    "# Initialize dictionaries to store subjects for specific verbs\n",
    "subjects_for_gritar = Counter()\n",
    "subjects_for_preguntar = Counter()\n",
    "subjects_for_responder = Counter()\n",
    "\n",
    "# Iterate over the verb-subject pairs\n",
    "for verb, subjects in verb_subject_pairs:\n",
    "    if verb in common_verbs:\n",
    "        # Update subject counters for the specific verbs\n",
    "        if verb == \"gritar\":\n",
    "            subjects_for_gritar.update(subjects)\n",
    "        elif verb == \"preguntar\":\n",
    "            subjects_for_preguntar.update(subjects)\n",
    "        elif verb == \"responder\":\n",
    "            subjects_for_responder.update(subjects)\n",
    "\n",
    "# Get the three most common subjects for each specified verb\n",
    "top_subjects_gritar = subjects_for_gritar.most_common(3)\n",
    "top_subjects_preguntar = subjects_for_preguntar.most_common(3)\n",
    "top_subjects_responder = subjects_for_responder.most_common(3)\n",
    "\n",
    "# Print results\n",
    "print(\"Ten most common verbs:\", common_verbs)\n",
    "print(\"\\nTop subjects for 'gritar':\", top_subjects_gritar)\n",
    "print(\"Top subjects for 'preguntar':\", top_subjects_preguntar)\n",
    "print(\"Top subjects for 'responder':\", top_subjects_responder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906e3ba-9856-4393-9a72-b9b60e38c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We are now going to analyse the difference between foreground and background events, \n",
    "using the distinction between the two tenses indefinido (which is said to be used for events which advance the storyline) \n",
    "and the imperfecto (used for background circumstances and events).\n",
    "\n",
    "6) Repeat the extraction of verb-subject pairs for the verb forms in both tenses (they are distinguished in UD by the values Past and Imp of the feature Tense). Which verbs occur more than five times in this novel, but exclusively denote foreground or background events? Based on their translations, do the results make sense? What does the plot appear to be centered on?\n",
    "7) Can you conclude from your data about the most frequent subjects who the protagonists of the plot are? Are there conspicuous differences in the rankings of people who are the agents in foreground and background events?\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
