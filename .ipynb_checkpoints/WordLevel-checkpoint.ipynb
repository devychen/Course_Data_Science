{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75126338-1de6-4556-aa2f-4eff2c627464",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "**1.0 Import Lexicons** <br>\n",
    "Initially we intended to use LIWC lexicon dictionairies (download [here](https://pypi.org/project/liwc/), and install using `!pip install -U liwc`). But it would require considerable fee. Therefore, we turned to a free equivalent called EMPATH whose guideline could be accessed [here](https://github.com/Ejhfast/empath-client). If it's still not working sufficiently, we will try the [SEANCE](https://www.linguisticanalysistools.org/seance.html). <br>\n",
    "\n",
    "**1.1 Explore EMPATHY, finding what existing lexicons from EMPATHY could be adopted directly.** <br>\n",
    "Next we explore the EMPATHY. In [Yarkoni (2011)](https://www.sciencedirect.com/science/article/pii/S0092656610000541), the referential article, its Table 1 displays a correlation between LIWC lexicons and the five dimensions of Big-Five personalities. We use this table as a benchmark to filter the EMPATHY, find the available labels, and then apply them to our dataset. <br>\n",
    "\n",
    "**1.2 Use Spacy to add lexicons that we need but missing from EMPATHY.** <br>\n",
    "For those missing, some of them such as 1st, 2nd, 3rd person pronouns could be added by parsing with Spacy, but for some of them there is a lack of instrument. We will take that as a limitation of this study. This new lexicon as a substitute of LIWC, we will call it EMPATHYe (Empathy extended).<br> \n",
    "\n",
    "**The results shows that:** <br>\n",
    "**EMPATHY has:** <br>\n",
    "affect, positive_emotions, negative_emotions, anger, sadness, hearing, communication, friends, family, swearing_terms <br>\n",
    "**Need Spacy for:** <br>\n",
    "pronouns(PRON), articles(DET), prepositions(PREP), numbers(NUM) <br>\n",
    "1st person sg/pl, 2nd person, 3rd person pronouns <br>\n",
    "Past/present/future tense vb. <br>\n",
    "And we **neglect the rest.** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0822a4dd-ab01-4254-8101-04dd13574065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', 'office', 'dance', 'money', 'wedding', 'domestic_work', 'sleep', 'medical_emergency', 'cold', 'hate', 'cheerfulness', 'aggression', 'occupation', 'envy', 'anticipation', 'family', 'vacation', 'crime', 'attractive', 'masculine', 'prison', 'health', 'pride', 'dispute', 'nervousness', 'government', 'weakness', 'horror', 'swearing_terms', 'leisure', 'suffering', 'royalty', 'wealthy', 'tourism', 'furniture', 'school', 'magic', 'beach', 'journalism', 'morning', 'banking', 'social_media', 'exercise', 'night', 'kill', 'blue_collar_job', 'art', 'ridicule', 'play', 'computer', 'college', 'optimism', 'stealing', 'real_estate', 'home', 'divine', 'sexual', 'fear', 'irritability', 'superhero', 'business', 'driving', 'pet', 'childish', 'cooking', 'exasperation', 'religion', 'hipster', 'internet', 'surprise', 'reading', 'worship', 'leader', 'independence', 'movement', 'body', 'noise', 'eating', 'medieval', 'zest', 'confusion', 'water', 'sports', 'death', 'healing', 'legend', 'heroic', 'celebration', 'restaurant', 'violence', 'programming', 'dominant_heirarchical', 'military', 'neglect', 'swimming', 'exotic', 'love', 'hiking', 'communication', 'hearing', 'order', 'sympathy', 'hygiene', 'weather', 'anonymity', 'trust', 'ancient', 'deception', 'fabric', 'air_travel', 'fight', 'dominant_personality', 'music', 'vehicle', 'politeness', 'toy', 'farming', 'meeting', 'war', 'speaking', 'listen', 'urban', 'shopping', 'disgust', 'fire', 'tool', 'phone', 'gain', 'sound', 'injury', 'sailing', 'rage', 'science', 'work', 'appearance', 'valuable', 'warmth', 'youth', 'sadness', 'fun', 'emotional', 'joy', 'affection', 'traveling', 'fashion', 'ugliness', 'lust', 'shame', 'torment', 'economics', 'anger', 'politics', 'ship', 'clothing', 'car', 'strength', 'technology', 'breaking', 'shape_and_size', 'power', 'white_collar_job', 'animal', 'party', 'terrorism', 'smell', 'disappointment', 'poor', 'plant', 'pain', 'beauty', 'timidity', 'philosophy', 'negotiate', 'negative_emotion', 'cleaning', 'messaging', 'competing', 'law', 'friends', 'payment', 'achievement', 'alcohol', 'liquid', 'feminine', 'weapon', 'children', 'monster', 'ocean', 'giving', 'contentment', 'writing', 'rural', 'positive_emotion', 'musical']\n",
      "\n",
      "Matched categories: ['money', 'domestic_work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school', 'social_media', 'blue_collar_job', 'optimism', 'home', 'sexual', 'superhero', 'religion', 'body', 'eating', 'sports', 'death', 'communication', 'hearing', 'weather', 'music', 'sound', 'work', 'sadness', 'emotional', 'affection', 'anger', 'white_collar_job', 'negative_emotion', 'friends', 'achievement', 'positive_emotion', 'musical']\n",
      "\n",
      "Not matched categories: ['total pronouns', 'pron', 'first person sing.', 'first person', 'second person', 'third person', 'negation', 'assent', 'prep', 'prepositions', 'number', 'affect', 'positive', 'negative', 'anxiety', 'cognitive', 'causation', 'insight', 'discrepancy', 'inhibition', 'tentative', 'certainty', 'sensory', 'seeing', 'feeling', 'social', 'references', 'friend', 'human', 'time', 'tense', 'space', 'up', 'down', 'inclusive', 'exclusive', 'motion', 'job', 'achieve', 'sport', 'tv', 'movie', 'finance', 'metaphysics', 'physical', 'sex', 'eat', 'drink', 'groom', 'swear']\n"
     ]
    }
   ],
   "source": [
    "### 1.0 Import Lexicon pkg ###\n",
    "# !pip install empath spacy pandas\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "### 1.1 EXPLORING EMPATHY ###\n",
    "### WHAT EXISTING CLASSES IN EMPATHY COULD BE ADOPTED DIRECTLY ###\n",
    "# Print all category (class) names\n",
    "print(list(lexicon.cats.keys()))\n",
    "print()\n",
    "\n",
    "# Define the list of class (category) names we're looking for\n",
    "categories_to_check = [\"total pronouns\", \"pron\", \"first person sing.\", \"first person\", \"second person\", \"third person\",\n",
    " \"negation\", \"assent\", \"articles\", \"prep\", \"prepositions\", \"number\",\n",
    " \"affect\", \"positive\", \"optimism\", \"negative\", \"anxiety\", \"anger\", \"sadness\", \n",
    " \"cognitive\", \"causation\", \"insight\", \"discrepancy\", \"inhibition\", \"tentative\", \"certainty\", \n",
    " \"sensory\", \"seeing\", \"hearing\", \"feeling\", \"social\", \"communication\", \"references\",\n",
    " \"friend\", \"family\", \"human\", \"time\", \"tense\", \"space\", \"up\", \"down\", \n",
    " \"inclusive\", \"exclusive\", \"motion\", \"occupation\", \"school\", \"job\", \"work\", \"achieve\", \n",
    " \"leisure\", \"home\", \"sport\", \"tv\", \"movie\", \"music\", \"sound\", \"money\", \"finance\",\n",
    " \"metaphysics\", \"religion\", \"death\", \"physical\", \"body\", \"sexuality\", \"sex\", \"eat\", \"drink\", \"sleep\", \"groom\", \"swear\"]\n",
    "\n",
    "# Convert categories_to_check to lowercase for case-insensitive comparison\n",
    "categories_to_check_lower = [cat.lower() for cat in categories_to_check]\n",
    "\n",
    "# Find matching categories (substring match, case insensitive)\n",
    "matched_categories = [cat for cat in lexicon.cats if any(search_term in cat.lower() for search_term in categories_to_check_lower)]\n",
    "not_matched_categories = [cat for cat in categories_to_check if not any(search_term in cat.lower() for search_term in lexicon.cats.keys())]\n",
    "\n",
    "# Output matched and not matched categories\n",
    "print(\"Matched categories:\", matched_categories)\n",
    "print()\n",
    "print(\"Not matched categories:\", not_matched_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00766a35-53ff-43a7-aaff-8ddc02242ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sexual': 0.2, 'love': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# do an example analysis for light testing\n",
    "result = lexicon.analyze(\"he kiss the other person\", normalize=True)\n",
    "filtered_result = {category: value for category, value in result.items() if value > 0}\n",
    "print(filtered_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01f706-de99-4624-82f9-8f5f34227d71",
   "metadata": {},
   "source": [
    "# 2. Data Processing\n",
    "\n",
    "**Recall the hypotheses for word level**\n",
    "\n",
    "We want to see if the correlations between LIWC categories and Big Five personlity traits align with the trend in Yarkoni(2011). That is: <br>\n",
    "|   EMPATHYe   |Label Name| Neuroticism  | Extroversion |   Openness   |Agreeableness |Conscientiousness|\n",
    "|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n",
    "| pronouns     |*pronouns|      +       |      +       |       --     |       ++     |       -      |\n",
    "| 1st person sing.|*first_person_sg|   ++      |      +       |       -      |       +      |       0      |\n",
    "| 1st person plural|*first_person_pl|   -      |     ++       |       --     |       ++     |       +      |\n",
    "| 1st person   |*first_person||++|+|--|++|+|\n",
    "| 2nd person   |*second_person||--|++|--|+|0|\n",
    "| 3rd person   |*third_person|+|+|-|+|-|\n",
    "| negations    |*negations|++|-|--|-|--|\n",
    "| articles     |*articles|--|-|++|+|++|\n",
    "| prepositions |*prepositions|-|-|++|+|+|\n",
    "| numbers      |*numbers|-|--|--|++|+|\n",
    "| affect       |affection|+|+|--|+|-|\n",
    "| positive emotions|positive_emotion|-|++|--|++|+|\n",
    "| optimism    |optimism|--|+|0|++|++|\n",
    "| negative emotions|negative_emotion|++|+|0|--|--|\n",
    "| anger        |anger|++|+|+|--|--|\n",
    "| sadness      |sadness|++|+|-|+|--|\n",
    "| hearing      |hearing|+|++|--|+|--|\n",
    "| communication|communication|0|++|-|+|-|\n",
    "| friends      |friends|--|++|-|++|+|\n",
    "| family       |family|-|+|--|++|+|\n",
    "| past tense vb.|*past_tense|+|-|--|+|0|\n",
    "| present tense vb.|*present_tense|+|-|--|0|-|\n",
    "| future tense vb.|*future_tense|-|-|-|-|-|\n",
    "| occupation   |occupation|+|--|+|-|+|\n",
    "| school       |school|+|-|+|-|-|\n",
    "| job/work     |work|+|--|+|-|+|\n",
    "| achievement  |achievement|+|--|-|+|--|\n",
    "| leisure      |leisure|-|++|--|++|+|\n",
    "| home         |home|0|+|--|++|+|\n",
    "| sports       |sports|-|+|--|+|0|\n",
    "| music        |music|-|++|+|+|--|\n",
    "| money        |money|+|-|-|--|-|\n",
    "| religion     |religion|-|++|+|+|-|\n",
    "| death        |death|+|+|++|--|--|\n",
    "| body states  |body|+|++|-|++|-|\n",
    "| sexuality    |sexuality|+|++|0|++|-|\n",
    "| eating       |eating|-|+|--|+|-|\n",
    "| sleep        |sleep|++|-|--|++|-|\n",
    "| swearing words|swearing_terms|++|+|+|--|--|\n",
    "(_Label Name_ refers to its new name in our _EMPATHYe_)\n",
    "\n",
    "**Re-classification Needed**:<br>\n",
    "The following labels from _EMPATHY_ will be renamed/reclassified in our _EMPATHYe_ <br>\n",
    "1)Work: domestick_work, blue_collar_job, white_collar_job, work <br>\n",
    "2)Music: music, sound, musical <br>\n",
    "3)Sexuality: sexual <br>\n",
    "\n",
    "**Dataset**<br>\n",
    "Our dataset use the collection of the complete 8 series of *Harry Potter* film series. Originally we use only the first film but turns out it's not sufficient for a significant result, therefore we applied them all. <br>\n",
    "\n",
    "**Build our own lexicon: *EMPATHYe*** <br>\n",
    "After the pre-processing stage, each character's lines form a separate dataset. Currently each dataset has the following labels: tokens, frequencies, postags. Now we need to add a new label called \"*empathye*\", which contains the needed lexicon information. Some could be proceeded directly by EMPATHY, some as stated before, need further processing using Spacy. <br>\n",
    "**2.1 Handle Empathy** <br>\n",
    "**2.2 Apply Spacy** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5300aa92-6806-4c14-8a61-a276644b32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1 START WITH EMPATHY ###\n",
    "\n",
    "# labels to keep\n",
    "labels_to_keep = [\n",
    "    'money', 'domestic_work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school',\n",
    "    'blue_collar_job', 'optimism', 'home', 'sexual', 'superhero', 'religion', 'body', 'eating', 'sports',\n",
    "    'death', 'communication', 'hearing', 'music', 'sound', 'work', 'sadness', 'emotional', 'affection',\n",
    "    'anger', 'white_collar_job', 'negative_emotion', 'friends', 'achievement', 'positive_emotion', 'musical'\n",
    "]\n",
    "\n",
    "# merging rules:\n",
    "merge_rules = {\n",
    "    'work': ['domestic_work', 'blue_collar_job', 'white_collar_job', 'work'],\n",
    "    'music': ['music', 'sound', 'musical'],\n",
    "    'sexuality': ['sexual']\n",
    "}\n",
    "\n",
    "temp_lexicon = {} # temporarily store the lexicon\n",
    "\n",
    "# filter and merge based on the rules above\n",
    "for label in labels_to_keep:\n",
    "    # 检查标签是否需要合并\n",
    "    merged = False\n",
    "    for new_label, old_labels in merge_rules.items():\n",
    "        if label in old_labels:\n",
    "            # 如果是要合并的标签，将内容合并至新标签\n",
    "            if new_label not in temp_lexicon:\n",
    "                temp_lexicon[new_label] = set()\n",
    "            temp_lexicon[new_label].update(lexicon.cats[label])\n",
    "            merged = True\n",
    "            break\n",
    "    # 如果标签不在合并规则内，直接添加到临时存储中\n",
    "    if not merged:\n",
    "        temp_lexicon[label] = lexicon.cats[label]\n",
    "\n",
    "# Clear original lexicon contents\n",
    "lexicon.cats.clear()\n",
    "\n",
    "# Reassign the updated content to lexicon\n",
    "lexicon.cats.update(temp_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f80c9c6-a2b8-48cc-a931-007a3a4a354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['money', 'work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school', 'optimism', 'home', 'sexuality', 'superhero', 'religion', 'body', 'eating', 'sports', 'death', 'communication', 'hearing', 'music', 'sadness', 'emotional', 'affection', 'anger', 'negative_emotion', 'friends', 'achievement', 'positive_emotion', 'first_person_sg', 'first_person_pl', 'first_person', 'second_person', 'third_person']\n"
     ]
    }
   ],
   "source": [
    "### 2.2 USE SPACY TO PROCEED MORE\n",
    "\n",
    "# [1] PERSONAL PRONOUNS\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load the English model\n",
    "# 定义代词标签\n",
    "pronouns = {\n",
    "    \"first_person_sg\": [\"I\", \"me\", \"my\", \"mine\"],\n",
    "    \"first_person_pl\": [\"we\", \"us\", \"our\", \"ours\"],\n",
    "    \"first_person\": [\"I\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"],\n",
    "    \"second_person\": [\"you\", \"your\", \"yours\"],\n",
    "    \"third_person\": [\"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"they\", \"them\", \"their\", \"theirs\"],\n",
    "}\n",
    "\n",
    "# 将 pronouns 添加到 lexicon\n",
    "for label, words in pronouns.items():\n",
    "    lexicon.cats[label] = words\n",
    "\n",
    "# 检查添加后的 lexicon\n",
    "print(list(lexicon.cats.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe1156c6-8bda-4506-9622-682d40361c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function for finding past, present, future tense words;\n",
    "# for finding numbers, prepositions, articles, negations\n",
    "\n",
    "# tense verbs\n",
    "def label_tenses(file_path):\n",
    "    # 读取 CSV 文件，不指定列名\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # 存储结果\n",
    "    labeled_verbs = {\n",
    "        'past_tense': [],\n",
    "        'present_tense': [],\n",
    "        'future_tense': []\n",
    "    }\n",
    "\n",
    "    # 遍历每一行文本\n",
    "    for index in range(len(df)):\n",
    "        # 使用 spaCy 处理每一行文本\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        # 查找动词\n",
    "        for token in doc:\n",
    "            # if token.pos_ == \"VERB\":  # 确保是动词\n",
    "                # 根据时态分类\n",
    "                if token.tag_ in ['VBD', 'VBN']:  # 过去时动词\n",
    "                    labeled_verbs['past_tense'].append(token.text)\n",
    "                elif token.tag_ in ['VBZ', 'VBP', 'VBG']:  # 现在时动词\n",
    "                    labeled_verbs['present_tense'].append(token.text)\n",
    "                elif token.tag_ == 'MD':  # 将来时动词（情态动词）\n",
    "                    # labeled_verbs['future_tense'].append(token.nbor().text)\n",
    "                    # 需要检查下一个词是否为动词以确定将来时\n",
    "                     if token.nbor().pos_ == \"VERB\":\n",
    "                        labeled_verbs['future_tense'].append(token.nbor().text)\n",
    "\n",
    "    return labeled_verbs\n",
    "\n",
    "# numbers\n",
    "def label_numbers(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_numbers = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.like_num:  # 判断是否是数字\n",
    "                labeled_numbers.append(token.text)\n",
    "\n",
    "    return labeled_numbers\n",
    "\n",
    "\n",
    "# prepositions\n",
    "\n",
    "def label_prepositions(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_prepositions = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"ADP\":  # 介词的 POS 标签是 ADP\n",
    "                labeled_prepositions.append(token.text)\n",
    "\n",
    "    return labeled_prepositions\n",
    "\n",
    "\n",
    "# articles\n",
    "\n",
    "def label_articles(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_articles = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"DET\":  # 冠词的 POS 标签是 DET\n",
    "                labeled_articles.append(token.text)\n",
    "\n",
    "    return labeled_articles\n",
    "\n",
    "\n",
    "# negations\n",
    "\n",
    "def label_negations(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_negations = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"neg\":  # 否定词的依存关系标签是 neg\n",
    "                labeled_negations.append(token.text)\n",
    "\n",
    "    return labeled_negations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea98e49-a355-4a86-a847-ce537ecd08ba",
   "metadata": {},
   "source": [
    "# 3. Data Analysis\n",
    "\n",
    "**Find the personality** <br> \n",
    "Based on the lexicons, we start the data analysis of a character's personalty. <br>\n",
    "Step 1: count the frequencies per lexicon. <br>\n",
    "Step 2: compare the lexicon frequencies between characters, using percentage = freq / number_of_tokens <br>\n",
    "Step 3: verify the hypotheses, i.e. based on the percentage we would know if characters being more extroverted characters tend to use more certain words compare to those less. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4e877e8-2706-464d-b3c3-f90aab3fc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tense_analysis(file_path):\n",
    "    # 调用不同的函数并打印结果\n",
    "    tenses_result = label_tenses(file_path)\n",
    "\n",
    "    # 计算总单词数\n",
    "    total_words = 0\n",
    "\n",
    "    # 计算每个句子的总单词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "        total_words += len(doc)  # 统计当前句子的单词数\n",
    "\n",
    "    # 获取各类动词的数量\n",
    "    past_count = len(tenses_result['past_tense'])\n",
    "    present_count = len(tenses_result['present_tense'])\n",
    "    future_count = len(tenses_result['future_tense'])\n",
    "\n",
    "    # 计算百分比\n",
    "    past_percentage = (past_count / total_words) * 100 if total_words > 0 else 0\n",
    "    present_percentage = (present_count / total_words) * 100 if total_words > 0 else 0\n",
    "    future_percentage = (future_count / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Past Tense Verbs: {past_count} ({past_percentage:.2f}%)\")\n",
    "    print(f\"Present Tense Verbs: {present_count} ({present_percentage:.2f}%)\")\n",
    "    print(f\"Future Tense Verbs: {future_count} ({future_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fd2ce0a-158d-4742-95c2-769e75270e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past Tense Verbs: 281 (2.94%)\n",
      "Present Tense Verbs: 362 (3.79%)\n",
      "Future Tense Verbs: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "### DUMBLEDORE ###\n",
    "process_tense_analysis('tokens/Dumbledore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069066f2-1c78-46ea-a07b-ea9d6a1436eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HARRY ###\n",
    "process_tense_analysis('tokens/Harry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7014ba-cd2f-43d0-be9a-b39a86f3b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HERMIONE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
