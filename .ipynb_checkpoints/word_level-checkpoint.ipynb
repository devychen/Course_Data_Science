{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75126338-1de6-4556-aa2f-4eff2c627464",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "**1.0 Import Lexicons** <br>\n",
    "Initially we intended to use LIWC lexicon dictionairies (download [here](https://pypi.org/project/liwc/), and install using `!pip install -U liwc`). But it would require considerable fee. Therefore, we turned to a free equivalent called EMPATH whose guideline could be accessed [here](https://github.com/Ejhfast/empath-client). If it's still not working sufficiently, we will try the [SEANCE](https://www.linguisticanalysistools.org/seance.html). <br>\n",
    "\n",
    "**1.1 Explore EMPATHY, finding what existing lexicons from EMPATHY could be adopted directly.** <br>\n",
    "Next we explore the EMPATHY. In [Yarkoni (2011)](https://www.sciencedirect.com/science/article/pii/S0092656610000541), the referential article, its Table 1 displays a correlation between LIWC lexicons and the five dimensions of Big-Five personalities. We use this table as a benchmark to filter the EMPATHY, find the available labels, and then apply them to our dataset. <br>\n",
    "\n",
    "**1.2 Use Spacy to add lexicons that we need but missing from EMPATHY.** <br>\n",
    "For those missing, some of them such as 1st, 2nd, 3rd person pronouns could be added by parsing with Spacy, but for some of them there is a lack of instrument. We will take that as a limitation of this study. This new lexicon as a substitute of LIWC, we will call it EMPATHYe (Empathy extended).<br> \n",
    "\n",
    "**The results shows that:** <br>\n",
    "**EMPATHY has:** <br>\n",
    "affect, positive_emotions, negative_emotions, anger, sadness, hearing, communication, friends, family, swearing_terms <br>\n",
    "**Need Spacy for:** <br>\n",
    "pronouns(PRON), articles(DET), prepositions(PREP), numbers(NUM) <br>\n",
    "1st person sg/pl, 2nd person, 3rd person pronouns <br>\n",
    "Past/present/future tense vb. <br>\n",
    "And we **neglect the rest.** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0822a4dd-ab01-4254-8101-04dd13574065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', 'office', 'dance', 'money', 'wedding', 'domestic_work', 'sleep', 'medical_emergency', 'cold', 'hate', 'cheerfulness', 'aggression', 'occupation', 'envy', 'anticipation', 'family', 'vacation', 'crime', 'attractive', 'masculine', 'prison', 'health', 'pride', 'dispute', 'nervousness', 'government', 'weakness', 'horror', 'swearing_terms', 'leisure', 'suffering', 'royalty', 'wealthy', 'tourism', 'furniture', 'school', 'magic', 'beach', 'journalism', 'morning', 'banking', 'social_media', 'exercise', 'night', 'kill', 'blue_collar_job', 'art', 'ridicule', 'play', 'computer', 'college', 'optimism', 'stealing', 'real_estate', 'home', 'divine', 'sexual', 'fear', 'irritability', 'superhero', 'business', 'driving', 'pet', 'childish', 'cooking', 'exasperation', 'religion', 'hipster', 'internet', 'surprise', 'reading', 'worship', 'leader', 'independence', 'movement', 'body', 'noise', 'eating', 'medieval', 'zest', 'confusion', 'water', 'sports', 'death', 'healing', 'legend', 'heroic', 'celebration', 'restaurant', 'violence', 'programming', 'dominant_heirarchical', 'military', 'neglect', 'swimming', 'exotic', 'love', 'hiking', 'communication', 'hearing', 'order', 'sympathy', 'hygiene', 'weather', 'anonymity', 'trust', 'ancient', 'deception', 'fabric', 'air_travel', 'fight', 'dominant_personality', 'music', 'vehicle', 'politeness', 'toy', 'farming', 'meeting', 'war', 'speaking', 'listen', 'urban', 'shopping', 'disgust', 'fire', 'tool', 'phone', 'gain', 'sound', 'injury', 'sailing', 'rage', 'science', 'work', 'appearance', 'valuable', 'warmth', 'youth', 'sadness', 'fun', 'emotional', 'joy', 'affection', 'traveling', 'fashion', 'ugliness', 'lust', 'shame', 'torment', 'economics', 'anger', 'politics', 'ship', 'clothing', 'car', 'strength', 'technology', 'breaking', 'shape_and_size', 'power', 'white_collar_job', 'animal', 'party', 'terrorism', 'smell', 'disappointment', 'poor', 'plant', 'pain', 'beauty', 'timidity', 'philosophy', 'negotiate', 'negative_emotion', 'cleaning', 'messaging', 'competing', 'law', 'friends', 'payment', 'achievement', 'alcohol', 'liquid', 'feminine', 'weapon', 'children', 'monster', 'ocean', 'giving', 'contentment', 'writing', 'rural', 'positive_emotion', 'musical']\n",
      "\n",
      "Matched categories: ['money', 'domestic_work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school', 'social_media', 'blue_collar_job', 'optimism', 'home', 'sexual', 'superhero', 'religion', 'body', 'eating', 'sports', 'death', 'communication', 'hearing', 'weather', 'music', 'sound', 'work', 'sadness', 'emotional', 'affection', 'anger', 'white_collar_job', 'negative_emotion', 'friends', 'achievement', 'positive_emotion', 'musical']\n",
      "\n",
      "Not matched categories: ['total pronouns', 'pron', 'first person sing.', 'first person', 'second person', 'third person', 'negation', 'assent', 'prep', 'prepositions', 'number', 'affect', 'positive', 'negative', 'anxiety', 'cognitive', 'causation', 'insight', 'discrepancy', 'inhibition', 'tentative', 'certainty', 'sensory', 'seeing', 'feeling', 'social', 'references', 'friend', 'human', 'time', 'tense', 'space', 'up', 'down', 'inclusive', 'exclusive', 'motion', 'job', 'achieve', 'sport', 'tv', 'movie', 'finance', 'metaphysics', 'physical', 'sex', 'eat', 'drink', 'groom', 'swear']\n"
     ]
    }
   ],
   "source": [
    "### 1.0 Import Lexicon pkg ###\n",
    "# !pip install empath spacy pandas numpy scipy statsmodels\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "### 1.1 EXPLORING EMPATHY ###\n",
    "### WHAT EXISTING CLASSES IN EMPATHY COULD BE ADOPTED DIRECTLY ###\n",
    "# Print all category (class) names\n",
    "print(list(lexicon.cats.keys()))\n",
    "print()\n",
    "\n",
    "# Define the list of class (category) names we're looking for\n",
    "categories_to_check = [\"total pronouns\", \"pron\", \"first person sing.\", \"first person\", \"second person\", \"third person\",\n",
    " \"negation\", \"assent\", \"articles\", \"prep\", \"prepositions\", \"number\",\n",
    " \"affect\", \"positive\", \"optimism\", \"negative\", \"anxiety\", \"anger\", \"sadness\", \n",
    " \"cognitive\", \"causation\", \"insight\", \"discrepancy\", \"inhibition\", \"tentative\", \"certainty\", \n",
    " \"sensory\", \"seeing\", \"hearing\", \"feeling\", \"social\", \"communication\", \"references\",\n",
    " \"friend\", \"family\", \"human\", \"time\", \"tense\", \"space\", \"up\", \"down\", \n",
    " \"inclusive\", \"exclusive\", \"motion\", \"occupation\", \"school\", \"job\", \"work\", \"achieve\", \n",
    " \"leisure\", \"home\", \"sport\", \"tv\", \"movie\", \"music\", \"sound\", \"money\", \"finance\",\n",
    " \"metaphysics\", \"religion\", \"death\", \"physical\", \"body\", \"sexuality\", \"sex\", \"eat\", \"drink\", \"sleep\", \"groom\", \"swear\"]\n",
    "\n",
    "# Convert categories_to_check to lowercase for case-insensitive comparison\n",
    "categories_to_check_lower = [cat.lower() for cat in categories_to_check]\n",
    "\n",
    "# Find matching categories (substring match, case insensitive)\n",
    "matched_categories = [cat for cat in lexicon.cats if any(search_term in cat.lower() for search_term in categories_to_check_lower)]\n",
    "not_matched_categories = [cat for cat in categories_to_check if not any(search_term in cat.lower() for search_term in lexicon.cats.keys())]\n",
    "\n",
    "# Output matched and not matched categories\n",
    "print(\"Matched categories:\", matched_categories)\n",
    "print()\n",
    "print(\"Not matched categories:\", not_matched_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00766a35-53ff-43a7-aaff-8ddc02242ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sexual': 0.2, 'love': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# do an example analysis for light testing\n",
    "result = lexicon.analyze(\"he kiss the other person\", normalize=True)\n",
    "filtered_result = {category: value for category, value in result.items() if value > 0}\n",
    "print(filtered_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01f706-de99-4624-82f9-8f5f34227d71",
   "metadata": {},
   "source": [
    "# 2. Data Processing\n",
    "\n",
    "**Recall the hypotheses for word level**\n",
    "\n",
    "We want to see if the correlations between LIWC categories and Big Five personlity traits align with the trend in Yarkoni(2011). That is: <br>\n",
    "|   EMPATHYe   |Label Name| Neuroticism  | Extroversion |   Openness   |Agreeableness |Conscientiousness|\n",
    "|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n",
    "| pronouns     |*pronouns|      +       |      +       |       --     |       ++     |       -      |\n",
    "| 1st person sing.|*first_person_sg|   ++      |      +       |       -      |       +      |       0      |\n",
    "| 1st person plural|*first_person_pl|   -      |     ++       |       --     |       ++     |       +      |\n",
    "| 1st person   |*first_person||++|+|--|++|+|\n",
    "| 2nd person   |*second_person||--|++|--|+|0|\n",
    "| 3rd person   |*third_person|+|+|-|+|-|\n",
    "| negations    |*negations|++|-|--|-|--|\n",
    "| articles     |*articles|--|-|++|+|++|\n",
    "| prepositions |*prepositions|-|-|++|+|+|\n",
    "| numbers      |*numbers|-|--|--|++|+|\n",
    "| affect       |affection|+|+|--|+|-|\n",
    "| positive emotions|positive_emotion|-|++|--|++|+|\n",
    "| optimism    |optimism|--|+|0|++|++|\n",
    "| negative emotions|negative_emotion|++|+|0|--|--|\n",
    "| anger        |anger|++|+|+|--|--|\n",
    "| sadness      |sadness|++|+|-|+|--|\n",
    "| hearing      |hearing|+|++|--|+|--|\n",
    "| communication|communication|0|++|-|+|-|\n",
    "| friends      |friends|--|++|-|++|+|\n",
    "| family       |family|-|+|--|++|+|\n",
    "| past tense vb.|*past_tense|+|-|--|+|0|\n",
    "| present tense vb.|*present_tense|+|-|--|0|-|\n",
    "| future tense vb.|*future_tense|-|-|-|-|-|\n",
    "| occupation   |occupation|+|--|+|-|+|\n",
    "| school       |school|+|-|+|-|-|\n",
    "| job/work     |work|+|--|+|-|+|\n",
    "| achievement  |achievement|+|--|-|+|--|\n",
    "| leisure      |leisure|-|++|--|++|+|\n",
    "| home         |home|0|+|--|++|+|\n",
    "| sports       |sports|-|+|--|+|0|\n",
    "| music        |music|-|++|+|+|--|\n",
    "| money        |money|+|-|-|--|-|\n",
    "| religion     |religion|-|++|+|+|-|\n",
    "| death        |death|+|+|++|--|--|\n",
    "| body states  |body|+|++|-|++|-|\n",
    "| sexuality    |sexuality|+|++|0|++|-|\n",
    "| eating       |eating|-|+|--|+|-|\n",
    "| sleep        |sleep|++|-|--|++|-|\n",
    "| swearing words|swearing_terms|++|+|+|--|--|\n",
    "(_Label Name_ refers to its new name in our _EMPATHYe_)\n",
    "\n",
    "**Re-classification Needed**:<br>\n",
    "The following labels from _EMPATHY_ will be renamed/reclassified in our _EMPATHYe_ <br>\n",
    "1)Work: domestick_work, blue_collar_job, white_collar_job, work <br>\n",
    "2)Music: music, sound, musical <br>\n",
    "3)Sexuality: sexual <br>\n",
    "\n",
    "**Dataset**<br>\n",
    "Our dataset use the collection of the complete 8 series of *Harry Potter* film series. Originally we use only the first film but turns out it's not sufficient for a significant result, therefore we applied them all. <br>\n",
    "\n",
    "**Build our own lexicon: *EMPATHYe*** <br>\n",
    "After the pre-processing stage, each character's lines form a separate dataset. Currently each dataset has the following labels: tokens, frequencies, postags. Now we need to add a new label called \"*empathye*\", which contains the needed lexicon information. Some could be proceeded directly by EMPATHY, some as stated before, need further processing using Spacy. <br>\n",
    "\n",
    "We follow the steps below:\n",
    "\n",
    "**2.1 Handle Empathy** <br>\n",
    "\n",
    "**2.2 Apply Spacy** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5300aa92-6806-4c14-8a61-a276644b32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1 START WITH EMPATHY ###\n",
    "\n",
    "# labels to keep\n",
    "labels_to_keep = [\n",
    "    'money', 'domestic_work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school',\n",
    "    'blue_collar_job', 'optimism', 'home', 'sexual', 'superhero', 'religion', 'body', 'eating', 'sports',\n",
    "    'death', 'communication', 'hearing', 'music', 'sound', 'work', 'sadness', 'emotional', 'affection',\n",
    "    'anger', 'white_collar_job', 'negative_emotion', 'friends', 'achievement', 'positive_emotion', 'musical'\n",
    "]\n",
    "\n",
    "# merging rules:\n",
    "merge_rules = {\n",
    "    'work': ['domestic_work', 'blue_collar_job', 'white_collar_job', 'work'],\n",
    "    'music': ['music', 'sound', 'musical'],\n",
    "    'sexuality': ['sexual']\n",
    "}\n",
    "\n",
    "temp_lexicon = {} # temporarily store the lexicon\n",
    "\n",
    "# filter and merge based on the rules above\n",
    "for label in labels_to_keep:\n",
    "    # 检查标签是否需要合并\n",
    "    merged = False\n",
    "    for new_label, old_labels in merge_rules.items():\n",
    "        if label in old_labels:\n",
    "            # 如果是要合并的标签，将内容合并至新标签\n",
    "            if new_label not in temp_lexicon:\n",
    "                temp_lexicon[new_label] = set()\n",
    "            temp_lexicon[new_label].update(lexicon.cats[label])\n",
    "            merged = True\n",
    "            break\n",
    "    # 如果标签不在合并规则内，直接添加到临时存储中\n",
    "    if not merged:\n",
    "        temp_lexicon[label] = lexicon.cats[label]\n",
    "\n",
    "# Clear original lexicon contents\n",
    "lexicon.cats.clear()\n",
    "\n",
    "# Reassign the updated content to lexicon\n",
    "lexicon.cats.update(temp_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f80c9c6-a2b8-48cc-a931-007a3a4a354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['money', 'work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school', 'optimism', 'home', 'sexuality', 'superhero', 'religion', 'body', 'eating', 'sports', 'death', 'communication', 'hearing', 'music', 'sadness', 'emotional', 'affection', 'anger', 'negative_emotion', 'friends', 'achievement', 'positive_emotion', 'first_person_sg', 'first_person_pl', 'first_person', 'second_person', 'third_person']\n"
     ]
    }
   ],
   "source": [
    "### 2.2 USE SPACY TO PROCEED MORE\n",
    "\n",
    "# [1] PERSONAL PRONOUNS\n",
    "nlp = spacy.load(\"en_core_web_lg\") # load the English model\n",
    "# 定义代词标签\n",
    "pronouns = {\n",
    "    \"first_person_sg\": [\"I\", \"me\", \"my\", \"mine\"],\n",
    "    \"first_person_pl\": [\"we\", \"us\", \"our\", \"ours\"],\n",
    "    \"first_person\": [\"I\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"],\n",
    "    \"second_person\": [\"you\", \"your\", \"yours\"],\n",
    "    \"third_person\": [\"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"they\", \"them\", \"their\", \"theirs\"],\n",
    "}\n",
    "\n",
    "# 将 pronouns 添加到 lexicon\n",
    "for label, words in pronouns.items():\n",
    "    lexicon.cats[label] = words\n",
    "\n",
    "# 检查添加后的 lexicon\n",
    "print(list(lexicon.cats.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe1156c6-8bda-4506-9622-682d40361c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function for finding past, present, future tense words;\n",
    "# for finding numbers, prepositions, articles, negations\n",
    "\n",
    "# tense verbs\n",
    "def label_tenses(file_path):\n",
    "    # 读取 CSV 文件，不指定列名\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # 存储结果\n",
    "    labeled_verbs = {\n",
    "        'past_tense': [],\n",
    "        'present_tense': [],\n",
    "        'future_tense': []\n",
    "    }\n",
    "\n",
    "    # 遍历每一行文本\n",
    "    for index in range(len(df)):\n",
    "        # 使用 spaCy 处理每一行文本\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        # 查找动词\n",
    "        for token in doc:\n",
    "            # if token.pos_ == \"VERB\":  # 确保是动词\n",
    "                # 根据时态分类\n",
    "                if token.tag_ in ['VBD', 'VBN']:  # 过去时动词\n",
    "                    labeled_verbs['past_tense'].append(token.text)\n",
    "                elif token.tag_ in ['VBZ', 'VBP', 'VBG']:  # 现在时动词\n",
    "                    labeled_verbs['present_tense'].append(token.text)\n",
    "                elif token.tag_ == 'MD':  # 将来时动词（情态动词）\n",
    "                    # labeled_verbs['future_tense'].append(token.nbor().text)\n",
    "                    # 需要检查下一个词是否为动词以确定将来时\n",
    "                     if token.nbor().pos_ == \"VERB\":\n",
    "                        labeled_verbs['future_tense'].append(token.nbor().text)\n",
    "\n",
    "    return labeled_verbs\n",
    "\n",
    "# numbers\n",
    "def label_numbers(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_numbers = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.like_num:  # 判断是否是数字\n",
    "                labeled_numbers.append(token.text)\n",
    "\n",
    "    return labeled_numbers\n",
    "\n",
    "\n",
    "# prepositions\n",
    "\n",
    "def label_prepositions(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_prepositions = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"ADP\":  # 介词的 POS 标签是 ADP\n",
    "                labeled_prepositions.append(token.text)\n",
    "\n",
    "    return labeled_prepositions\n",
    "\n",
    "\n",
    "# articles\n",
    "\n",
    "def label_articles(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_articles = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"DET\":  # 冠词的 POS 标签是 DET\n",
    "                labeled_articles.append(token.text)\n",
    "\n",
    "    return labeled_articles\n",
    "\n",
    "\n",
    "# negations\n",
    "\n",
    "def label_negations(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_negations = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"neg\":  # 否定词的依存关系标签是 neg\n",
    "                labeled_negations.append(token.text)\n",
    "\n",
    "    return labeled_negations\n",
    "\n",
    "\n",
    "def label_pronouns(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_pronouns = {\"first_person\": [], \"second_person\": [], \"third_person\": []}\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.text in pronouns[\"first_person\"]:\n",
    "                labeled_pronouns[\"first_person\"].append(token.text)\n",
    "            elif token.text in pronouns[\"second_person\"]:\n",
    "                labeled_pronouns[\"second_person\"].append(token.text)\n",
    "            elif token.text in pronouns[\"third_person\"]:\n",
    "                labeled_pronouns[\"third_person\"].append(token.text)\n",
    "\n",
    "    return labeled_pronouns\n",
    "\n",
    "def label_empathy(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_empathy = {label: [] for label in lexicon.cats.keys()}\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问每一行的文本\n",
    "        doc_text = doc.text.lower()  # 将文本转换为小写以匹配词汇表中的单词\n",
    "\n",
    "        for label, words in lexicon.cats.items():\n",
    "            for word in words:\n",
    "                if word in doc_text:\n",
    "                    labeled_empathy[label].append(word)\n",
    "\n",
    "    return labeled_empathy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea98e49-a355-4a86-a847-ce537ecd08ba",
   "metadata": {},
   "source": [
    "# 3. Data Processing\n",
    "\n",
    "Based on the lexicons, we start the data analysis of correlation between characters' personalties and their lines. <br>\n",
    "At this stage, we need to count the frequency and percentage of all labels per characters. Later on we will make the comparison based on their personality. <br>\n",
    "\n",
    "*percentage = freq / number_of_tokens\n",
    "\n",
    "Here is the **personality scores** from the reference study:\n",
    "\n",
    "|   Character   | Neuroticism  | Extroversion |   Openness   |Agreeableness |Conscientiousness|\n",
    "|--------------|--------------|--------------|--------------|--------------|--------------|\n",
    "|Ron Wesley|3.22|4.9|4.02|3.76|3.01|\n",
    "|Hermine Granger|4.22|4.65|5.12|4.07|6.22|\n",
    "|Albus Dumbledore|5.52|4.36|5.52|5.07|5.73|\n",
    "|Lord Voldmort|3|4.36|4.27|1.95|4.88|\n",
    "|Draco Malfoy|3.15|4.23|3.86|2.15|4.16|\n",
    "|Harry Potter|3.85|3.92|5.13|4.11|4.36|\n",
    "|Severus Snape|4.43|2.65|4.08|2.6|5.49|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4e877e8-2706-464d-b3c3-f90aab3fc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analysis_tense(file_path):\n",
    "    # 调用不同的函数并打印结果\n",
    "    tenses_result = label_tenses(file_path)\n",
    "\n",
    "    # 计算总单词数\n",
    "    total_words = 0\n",
    "\n",
    "    # 计算每个句子的总单词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "        total_words += len(doc)  # 统计当前句子的单词数\n",
    "\n",
    "    # 获取各类动词的数量\n",
    "    past_count = len(tenses_result['past_tense'])\n",
    "    present_count = len(tenses_result['present_tense'])\n",
    "    future_count = len(tenses_result['future_tense'])\n",
    "\n",
    "    # 计算百分比\n",
    "    past_percentage = (past_count / total_words) * 100 if total_words > 0 else 0\n",
    "    present_percentage = (present_count / total_words) * 100 if total_words > 0 else 0\n",
    "    future_percentage = (future_count / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Past Tense Verbs: {past_count} ({past_percentage:.2f}%)\")\n",
    "    print(f\"Present Tense Verbs: {present_count} ({present_percentage:.2f}%)\")\n",
    "    print(f\"Future Tense Verbs: {future_count} ({future_percentage:.2f}%)\")\n",
    "\n",
    "def analysis_numbers(file_path):\n",
    "    labeled_numbers = label_numbers(file_path)\n",
    "    \n",
    "    # 统计数字token的总数量\n",
    "    total_numbers = len(labeled_numbers)\n",
    "    \n",
    "    # 计算总词数（包含数字和非数字的总词数）\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 计算数字token的占比\n",
    "    number_percentage = (total_numbers / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Total number words: {total_numbers} ({number_percentage:.2f}%)\")\n",
    "    # print(f\"Total words containing numbers: {total_numbers}\")\n",
    "    # print(f\"Percentage of numbers in all tokens: {percentage:.2f}%\")\n",
    "\n",
    "\n",
    "# Analysis for prepositions\n",
    "def analysis_prepositions(file_path):\n",
    "    labeled_prepositions = label_prepositions(file_path)\n",
    "    \n",
    "    # 统计介词的总数量\n",
    "    total_prepositions = len(labeled_prepositions)\n",
    "    \n",
    "    # 计算总词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 计算介词的占比\n",
    "    preposition_percentage = (total_prepositions / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Total prepositions: {total_prepositions} ({preposition_percentage:.2f}%)\")\n",
    "\n",
    "\n",
    "# Analysis for articles\n",
    "def analysis_articles(file_path):\n",
    "    labeled_articles = label_articles(file_path)\n",
    "    \n",
    "    # 统计冠词的总数量\n",
    "    total_articles = len(labeled_articles)\n",
    "    \n",
    "    # 计算总词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 计算冠词的占比\n",
    "    article_percentage = (total_articles / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Total articles: {total_articles} ({article_percentage:.2f}%)\")\n",
    "\n",
    "\n",
    "# Analysis for negations\n",
    "def analysis_negations(file_path):\n",
    "    labeled_negations = label_negations(file_path)\n",
    "    \n",
    "    # 统计否定词的总数量\n",
    "    total_negations = len(labeled_negations)\n",
    "    \n",
    "    # 计算总词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 计算否定词的占比\n",
    "    negation_percentage = (total_negations / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Total negations: {total_negations} ({negation_percentage:.2f}%)\")\n",
    "\n",
    "def analysis_pronouns(file_path):\n",
    "    labeled_pronouns = label_pronouns(file_path)\n",
    "\n",
    "    # 统计每种代词的数量\n",
    "    first_person_count = len(labeled_pronouns[\"first_person\"])\n",
    "    second_person_count = len(labeled_pronouns[\"second_person\"])\n",
    "    third_person_count = len(labeled_pronouns[\"third_person\"])\n",
    "\n",
    "    # 计算总词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 计算各代词的百分比\n",
    "    first_person_percentage = (first_person_count / total_words) * 100 if total_words > 0 else 0\n",
    "    second_person_percentage = (second_person_count / total_words) * 100 if total_words > 0 else 0\n",
    "    third_person_percentage = (third_person_count / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"First Person Pronouns: {first_person_count} ({first_person_percentage:.2f}%)\")\n",
    "    print(f\"Second Person Pronouns: {second_person_count} ({second_person_percentage:.2f}%)\")\n",
    "    print(f\"Third Person Pronouns: {third_person_count} ({third_person_percentage:.2f}%)\")\n",
    "\n",
    "def analysis_empathy(file_path):\n",
    "    labeled_empathy = label_empathy(file_path)\n",
    "\n",
    "    # 统计每个类别的词频\n",
    "    category_counts = {label: len(words) for label, words in labeled_empathy.items()}\n",
    "    total_empathy_words = sum(category_counts.values())\n",
    "\n",
    "    # 计算总词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 打印每个类别的频率和百分比\n",
    "    print(f\"Total empathy-related words: {total_empathy_words} ({(total_empathy_words / total_words) * 100:.2f}% of total words)\")\n",
    "    for label, count in category_counts.items():\n",
    "        percentage = (count / total_words) * 100 if total_words > 0 else 0\n",
    "        print(f\"{label}, {count} ({percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0fd2ce0a-158d-4742-95c2-769e75270e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement analysis and output results:\n",
    "\n",
    "def analysis_all(file_path):\n",
    "    \n",
    "    print(\"Tense Analysis:\")\n",
    "    analysis_tense(file_path)\n",
    "    \n",
    "    print(\"\\nNumber Analysis:\")\n",
    "    analysis_numbers(file_path)\n",
    "    \n",
    "    print(\"\\nPreposition Analysis:\")\n",
    "    analysis_prepositions(file_path)\n",
    "    \n",
    "    print(\"\\nArticle Analysis:\")\n",
    "    analysis_articles(file_path)\n",
    "    \n",
    "    print(\"\\nNegation Analysis:\")\n",
    "    analysis_negations(file_path)\n",
    "\n",
    "    print(\"\\nPronoun Analysis:\")\n",
    "    analysis_pronouns(file_path)\n",
    "    \n",
    "    print(\"\\nEmpathy Analysis:\")\n",
    "    analysis_empathy(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1a8c83af-3012-4cca-ae55-76e4da1e447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts per character:\n",
      "\n",
      "Albus Dumbledore\n",
      "Tense Analysis:\n",
      "Past Tense Verbs: 386 (4.04%)\n",
      "Present Tense Verbs: 708 (7.42%)\n",
      "Future Tense Verbs: 109 (1.14%)\n",
      "\n",
      "Number Analysis:\n",
      "Total number words: 95 (1.00%)\n",
      "\n",
      "Preposition Analysis:\n",
      "Total prepositions: 619 (6.49%)\n",
      "\n",
      "Article Analysis:\n",
      "Total articles: 575 (6.03%)\n",
      "\n",
      "Negation Analysis:\n",
      "Total negations: 123 (1.29%)\n",
      "\n",
      "Pronoun Analysis:\n",
      "First Person Pronouns: 406 (4.25%)\n",
      "Second Person Pronouns: 297 (3.11%)\n",
      "Third Person Pronouns: 147 (1.54%)\n",
      "\n",
      "Empathy Analysis:\n",
      "Total empathy-related words: 5391 (56.49% of total words)\n",
      "money, 54 (0.57%)\n",
      "work, 1047 (10.97%)\n",
      "sleep, 48 (0.50%)\n",
      "occupation, 8 (0.08%)\n",
      "family, 92 (0.96%)\n",
      "swearing_terms, 31 (0.32%)\n",
      "leisure, 28 (0.29%)\n",
      "school, 76 (0.80%)\n",
      "optimism, 46 (0.48%)\n",
      "home, 62 (0.65%)\n",
      "sexuality, 10 (0.10%)\n",
      "superhero, 8 (0.08%)\n",
      "religion, 48 (0.50%)\n",
      "body, 82 (0.86%)\n",
      "eating, 121 (1.27%)\n",
      "sports, 111 (1.16%)\n",
      "death, 100 (1.05%)\n",
      "communication, 140 (1.47%)\n",
      "hearing, 118 (1.24%)\n",
      "music, 149 (1.56%)\n",
      "sadness, 51 (0.53%)\n",
      "emotional, 47 (0.49%)\n",
      "affection, 19 (0.20%)\n",
      "anger, 30 (0.31%)\n",
      "negative_emotion, 151 (1.58%)\n",
      "friends, 113 (1.18%)\n",
      "achievement, 109 (1.14%)\n",
      "positive_emotion, 99 (1.04%)\n",
      "first_person_sg, 259 (2.71%)\n",
      "first_person_pl, 387 (4.06%)\n",
      "first_person, 646 (6.77%)\n",
      "second_person, 369 (3.87%)\n",
      "third_person, 732 (7.67%)\n",
      "\n",
      "Harry Potter\n",
      "Tense Analysis:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "[E042] Error accessing `doc[0].nbor(1)`, for doc of length 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m analysis_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens/Dumbledore.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHarry Potter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43manalysis_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTokens/Harry.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHermione Granger\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m analysis_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens/Hermione.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[72], line 6\u001b[0m, in \u001b[0;36manalysis_all\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalysis_all\u001b[39m(file_path):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTense Analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43manalysis_tense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNumber Analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     analysis_numbers(file_path)\n",
      "Cell \u001b[0;32mIn[71], line 5\u001b[0m, in \u001b[0;36manalysis_tense\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalysis_tense\u001b[39m(file_path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# 调用不同的函数并打印结果\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     tenses_result \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_tenses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# 计算总单词数\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     total_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[70], line 32\u001b[0m, in \u001b[0;36mlabel_tenses\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 labeled_verbs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpresent_tense\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mtag_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMD\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# 将来时动词（情态动词）\u001b[39;00m\n\u001b[1;32m     30\u001b[0m                 \u001b[38;5;66;03m# labeled_verbs['future_tense'].append(token.nbor().text)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m                 \u001b[38;5;66;03m# 需要检查下一个词是否为动词以确定将来时\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m                  \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     33\u001b[0m                     labeled_verbs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture_tense\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mnbor()\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labeled_verbs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/tokens/token.pyx:205\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.nbor\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E042] Error accessing `doc[0].nbor(1)`, for doc of length 1."
     ]
    }
   ],
   "source": [
    "print(\"Counts per character:\")\n",
    "\n",
    "print(\"\\nAlbus Dumbledore\")\n",
    "analysis_all(\"Tokens/Dumbledore.csv\")\n",
    "\n",
    "print(\"\\nHarry Potter\")\n",
    "analysis_all(\"Tokens/Harry.csv\")\n",
    "\n",
    "print(\"\\nHermione Granger\")\n",
    "analysis_all(\"Tokens/Hermione.csv\")\n",
    "\n",
    "print(\"\\nDraco Malfoy\")\n",
    "analysis_all(\"Tokens/Malfoy.csv\")\n",
    "\n",
    "print(\"\\nRon Wesley\")\n",
    "analysis_all(\"Tokens/Ron.csv\")\n",
    "\n",
    "print(\"\\nSeverus Snape\")\n",
    "analysis_all(\"Tokens/Snape.csv\")\n",
    "\n",
    "print(\"\\nLord Voldemort\")\n",
    "analysis_all(\"Tokens/Voldemort.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764acf8-8f31-493f-bf52-5deea98b8d1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Results on Language Use per Character\n",
    "\n",
    "| Label                          | Albus Dumbledore | Harry Potter | Hermione Granger | Draco Malfoy | Ron Wesley | Severus Snape | Lord Voldemort |\n",
    "|--------------------------------|------------------|--------------|------------------|--------------|------------|---------------|----------------|\n",
    "| **Tense Analysis**             |                  |              |                  |              |            |               |                |\n",
    "| Past Tense Verbs               | 390 (4.09%)     | 907 (4.65%)  | 469 (4.50%)      | 75 (4.32%)   | 389 (4.02%)| 109 (3.63%)   | 80 (3.69%)     |\n",
    "| Present Tense Verbs            | 702 (7.36%)     | 1711 (8.78%) | 957 (9.18%)      | 156 (8.98%)  | 892 (9.21%)| 225 (7.49%)   | 149 (6.88%)    |\n",
    "| Future Tense Verbs             | 110 (1.15%)     | 159 (0.82%)  | 83 (0.80%)       | 16 (0.92%)   | 50 (0.52%) | 34 (1.13%)    | 31 (1.43%)     |\n",
    "| **Number Analysis**            |                  |              |                  |              |            |               |                |\n",
    "| Total number words             | 95 (1.00%)      | 133 (0.68%)  | 61 (0.59%)       | 12 (0.69%)   | 76 (0.78%) | 22 (0.73%)    | 14 (0.65%)     |\n",
    "| **Preposition Analysis**       |                  |              |                  |              |            |               |                |\n",
    "| Total prepositions             | 627 (6.57%)     | 910 (4.67%)  | 527 (5.06%)      | 90 (5.18%)   | 471 (4.86%)| 217 (7.22%)   | 111 (5.12%)    |\n",
    "| **Article Analysis**           |                  |              |                  |              |            |               |                |\n",
    "| Total articles                 | 575 (6.03%)     | 772 (3.96%)  | 447 (4.29%)      | 82 (4.72%)   | 399 (4.12%)| 175 (5.82%)   | 84 (3.88%)     |\n",
    "| **Negation Analysis**          |                  |              |                  |              |            |               |                |\n",
    "| Total negations                | 123 (1.29%)     | 375 (1.92%)  | 208 (2.00%)      | 39 (2.24%)   | 186 (1.92%)| 38 (1.26%)    | 37 (1.71%)     |\n",
    "| **Pronoun Analysis**           |                  |              |                  |              |            |               |                |\n",
    "| First Person Pronouns          | 406 (4.25%)     | 1030 (5.28%) | 386 (3.70%)      | 90 (5.18%)   | 425 (4.39%)| 102 (3.39%)   | 141 (6.51%)    |\n",
    "| Second Person Pronouns         | 297 (3.11%)     | 486 (2.49%)  | 268 (2.57%)      | 65 (3.74%)   | 239 (2.47%)| 119 (3.96%)   | 83 (3.83%)     |\n",
    "| Third Person Pronouns          | 147 (1.54%)     | 358 (1.84%)  | 169 (1.62%)      | 21 (1.21%)   | 191 (1.97%)| 41 (1.36%)    | 37 (1.71%)     |\n",
    "| **Empathy Analysis**           |                  |              |                  |              |            |               |                |\n",
    "| Total empathy-related words    | 5391 (56.49%)   | 10603 (54.39%)| 5464 (52.42%)    | 1079 (62.08%)| 5179 (53.47%)| 1662 (55.31%) | 1371 (63.30%)  |\n",
    "| money                          | 54 (0.57%)      | 72 (0.37%)   | 47 (0.45%)       | 3 (0.17%)    | 28 (0.29%) | 12 (0.40%)    | 11 (0.51%)     |\n",
    "| work                           | 1047 (10.97%)   | 3105 (15.93%)| 1469 (14.09%)    | 241 (13.87%) | 1498 (15.47%)| 336 (11.18%) | 272 (12.56%)   |\n",
    "| sleep                          | 48 (0.50%)      | 80 (0.41%)   | 44 (0.42%)       | 4 (0.23%)    | 47 (0.49%) | 16 (0.53%)    | 6 (0.28%)      |\n",
    "| occupation                     | 8 (0.08%)       | 7 (0.04%)    | 11 (0.11%)       | 0 (0.00%)    | 7 (0.07%)  | 4 (0.13%)     | 2 (0.09%)      |\n",
    "| family                         | 92 (0.96%)      | 123 (0.63%)  | 67 (0.64%)       | 25 (1.44%)   | 72 (0.74%) | 22 (0.73%)    | 24 (1.11%)     |\n",
    "| swearing_terms                 | 31 (0.32%)      | 58 (0.30%)   | 35 (0.34%)       | 9 (0.52%)    | 62 (0.64%) | 11 (0.37%)    | 3 (0.14%)      |\n",
    "| leisure                        | 28 (0.29%)      | 24 (0.12%)   | 23 (0.22%)       | 5 (0.29%)    | 17 (0.18%) | 8 (0.27%)     | 3 (0.14%)      |\n",
    "| school                         | 76 (0.80%)      | 44 (0.23%)   | 49 (0.47%)       | 11 (0.63%)   | 17 (0.18%) | 21 (0.70%)    | 4 (0.18%)      |\n",
    "| optimism                       | 46 (0.48%)      | 49 (0.25%)   | 49 (0.47%)       | 9 (0.52%)    | 31 (0.32%) | 16 (0.53%)    | 12 (0.55%)     |\n",
    "| home                           | 62 (0.65%)      | 147 (0.75%)  | 58 (0.56%)       | 22 (1.27%)   | 76 (0.78%) | 23 (0.77%)    | 26 (1.20%)     |\n",
    "| sexuality                      | 10 (0.10%)      | 9 (0.05%)    | 14 (0.13%)       | 0 (0.00%)    | 6 (0.06%)  | 2 (0.07%)     | 4 (0.18%)      |\n",
    "| superhero                      | 8 (0.08%)       | 10 (0.05%)   | 11 (0.11%)       | 1 (0.06%)    | 4 (0.04%)  | 2 (0.07%)     | 1 (0.05%)      |\n",
    "| religion                       | 48 (0.50%)      | 79 (0.41%)   | 21 (0.20%)       | 6 (0.35%)    | 10 (0.10%) | 12 (0.40%)    | 5 (0.23%)      |\n",
    "| body                           | 82 (0.86%)      | 120 (0.62%)  | 75 (0.72%)       | 24 (1.38%)   | 66 (0.68%) | 31 (1.03%)    | 25 (1.15%)     |\n",
    "| eating                         | 121 (1.27%)     | 117 (0.60%)  | 73 (0.70%)       | 9 (0.52%)    | 74 (0.76%) | 20 (0.67%)    | 25 (1.15%)     |\n",
    "| sports                         | 111 (1.16%)     | 125 (0.64%)  | 49 (0.47%)       | 11 (0.63%)   | 45 (0.46%) | 16 (0.53%)    | 10 (0.46%)     |\n",
    "| death                          | 100 (1.05%)     | 119 (0.61%)  | 98 (0.94%)       | 6 (0.35%)    | 87 (0.90%) | 39 (1.30%)    | 14 (0.65%)     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d116b-6689-4d61-9ea1-6d481496e560",
   "metadata": {},
   "source": [
    "# 4. Data Analysis\n",
    "\n",
    "Now we run the statistical tests to test our hypotheses. <br>\n",
    "\n",
    "**Normal Distribution** <br>\n",
    "Before deciding which correlation test (Spearman or Pearson) to apply later, we first need to explore if our data is normally distributed. <br>\n",
    "\n",
    "**Test on Differences Between Characters**\n",
    "\n",
    "\n",
    "**Test on Correlation**\n",
    "As the result shows that only \"total prepositions\" and \"sports\" are not normally distributed, we will use _Spearman_ test for these two, and _Pearson_ for the rest to test the correlation. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc294eca-6127-41f7-8cd0-101d76de28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for statistical analysis\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1d9b8-c8ec-46f8-a54d-3973256b3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIGNIFICANT TEST ON CORRELATION ###\n",
    "\n",
    "\n",
    "# 数据字典\n",
    "data = {\n",
    "    'Character': ['Albus Dumbledore', 'Harry Potter', 'Hermione Granger', 'Draco Malfoy', \n",
    "                  'Ron Wesley', 'Severus Snape', 'Lord Voldemort'],\n",
    "    'Past Tense Verbs': [4.09, 4.65, 4.50, 4.32, 4.02, 3.63, 3.69],\n",
    "    'Present Tense Verbs': [7.36, 8.78, 9.18, 8.98, 9.21, 7.49, 6.88],\n",
    "    'Future Tense Verbs': [1.15, 0.82, 0.80, 0.92, 0.52, 1.13, 1.43],\n",
    "    'Total number words': [1.00, 0.68, 0.59, 0.69, 0.78, 0.73, 0.65],\n",
    "    'Total prepositions': [6.57, 4.67, 5.06, 5.18, 4.86, 7.22, 5.12],\n",
    "    'Total articles': [6.03, 3.96, 4.29, 4.72, 4.12, 5.82, 3.88],\n",
    "    'Total negations': [1.29, 1.92, 2.00, 2.24, 1.92, 1.26, 1.71],\n",
    "    'First Person Pronouns': [4.25, 5.28, 3.70, 5.18, 4.39, 3.39, 6.51],\n",
    "    'Second Person Pronouns': [3.11, 2.49, 2.57, 3.74, 2.47, 3.96, 3.83],\n",
    "    'Third Person Pronouns': [1.54, 1.84, 1.62, 1.21, 1.97, 1.36, 1.71],\n",
    "    'Total empathy-related words': [56.49, 54.39, 52.42, 62.08, 53.47, 55.31, 63.30],\n",
    "    'money': [0.57, 0.37, 0.45, 0.17, 0.29, 0.40, 0.51],\n",
    "    'work': [10.97, 15.93, 14.09, 13.87, 15.47, 11.18, 12.56],\n",
    "    'sleep': [0.50, 0.41, 0.42, 0.23, 0.49, 0.53, 0.28],\n",
    "    'occupation': [0.08, 0.04, 0.11, 0.00, 0.07, 0.13, 0.09],\n",
    "    'family': [0.96, 0.63, 0.64, 1.44, 0.74, 0.73, 1.11],\n",
    "    'swearing_terms': [0.32, 0.30, 0.34, 0.52, 0.64, 0.37, 0.14],\n",
    "    'leisure': [0.29, 0.12, 0.22, 0.29, 0.18, 0.27, 0.14],\n",
    "    'school': [0.80, 0.23, 0.47, 0.63, 0.18, 0.70, 0.18],\n",
    "    'optimism': [0.48, 0.25, 0.47, 0.52, 0.32, 0.53, 0.55],\n",
    "    'home': [0.65, 0.75, 0.56, 1.27, 0.78, 0.77, 1.20],\n",
    "    'sexuality': [0.10, 0.05, 0.13, 0.00, 0.06, 0.07, 0.18],\n",
    "    'superhero': [0.08, 0.05, 0.11, 0.06, 0.04, 0.07, 0.05],\n",
    "    'religion': [0.50, 0.41, 0.20, 0.35, 0.10, 0.40, 0.23],\n",
    "    'body': [0.86, 0.62, 0.72, 1.38, 0.68, 1.03, 1.15],\n",
    "    'eating': [1.27, 0.60, 0.70, 0.52, 0.76, 0.67, 1.15],\n",
    "    'sports': [1.16, 0.64, 0.47, 0.63, 0.46, 0.53, 0.46],\n",
    "    'death': [1.05, 0.61, 0.94, 0.35, 0.90, 1.30, 0.65],\n",
    "}\n",
    "\n",
    "# 创建数据框\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 定义一个函数来进行ANOVA测试\n",
    "def run_anova(df, column):\n",
    "    f_value, p_value = stats.f_oneway(*[df[column][df['Character'] == character] for character in df['Character']])\n",
    "    return f_value, p_value\n",
    "\n",
    "# 选择需要进行ANOVA测试的列\n",
    "columns_to_test = df.columns[1:]\n",
    "\n",
    "# 存储结果\n",
    "results = {}\n",
    "\n",
    "# 对每一列进行ANOVA测试\n",
    "for column in columns_to_test:\n",
    "    f_value, p_value = run_anova(df, column)\n",
    "    results[column] = {'F-value': f_value, 'p-value': p_value}\n",
    "\n",
    "# 打印结果\n",
    "for column, result in results.items():\n",
    "    print(f\"{column}: F-value = {result['F-value']:.4f}, p-value = {result['p-value']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f8570-4ee3-44ed-8c5b-ece4df03e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISTRIBUTION TEST ON LEXICON DATA ###\n",
    "\n",
    "# Defining the data for each category based on the percentages provided for each character\n",
    "data = {\n",
    "    \"Past Tense Verbs\": [4.09, 4.65, 4.50, 4.32, 4.02, 3.63, 3.69],\n",
    "    \"Present Tense Verbs\": [7.36, 8.78, 9.18, 8.98, 9.21, 7.49, 6.88],\n",
    "    \"Future Tense Verbs\": [1.15, 0.82, 0.80, 0.92, 0.52, 1.13, 1.43],\n",
    "    \"Total number words\": [1.00, 0.68, 0.59, 0.69, 0.78, 0.73, 0.65],\n",
    "    \"Total prepositions\": [6.57, 4.67, 5.06, 5.18, 4.86, 7.22, 5.12],\n",
    "    \"Total articles\": [6.03, 3.96, 4.29, 4.72, 4.12, 5.82, 3.88],\n",
    "    \"Total negations\": [1.29, 1.92, 2.00, 2.24, 1.92, 1.26, 1.71],\n",
    "    \"First Person Pronouns\": [4.25, 5.28, 3.70, 5.18, 4.39, 3.39, 6.51],\n",
    "    \"Second Person Pronouns\": [3.11, 2.49, 2.57, 3.74, 2.47, 3.96, 3.83],\n",
    "    \"Third Person Pronouns\": [1.54, 1.84, 1.62, 1.21, 1.97, 1.36, 1.71],\n",
    "    \"Total empathy-related words\": [56.49, 54.39, 52.42, 62.08, 53.47, 55.31, 63.30],\n",
    "    \"money\": [0.57, 0.37, 0.45, 0.17, 0.29, 0.40, 0.51],\n",
    "    \"work\": [10.97, 15.93, 14.09, 13.87, 15.47, 11.18, 12.56],\n",
    "    \"sleep\": [0.50, 0.41, 0.42, 0.23, 0.49, 0.53, 0.28],\n",
    "    \"occupation\": [0.08, 0.04, 0.11, 0.00, 0.07, 0.13, 0.09],\n",
    "    \"family\": [0.96, 0.63, 0.64, 1.44, 0.74, 0.73, 1.11],\n",
    "    \"swearing_terms\": [0.32, 0.30, 0.34, 0.52, 0.64, 0.37, 0.14],\n",
    "    \"leisure\": [0.29, 0.12, 0.22, 0.29, 0.18, 0.27, 0.14],\n",
    "    \"school\": [0.80, 0.23, 0.47, 0.63, 0.18, 0.70, 0.18],\n",
    "    \"optimism\": [0.48, 0.25, 0.47, 0.52, 0.32, 0.53, 0.55],\n",
    "    \"home\": [0.65, 0.75, 0.56, 1.27, 0.78, 0.77, 1.20],\n",
    "    \"sexuality\": [0.10, 0.05, 0.13, 0.00, 0.06, 0.07, 0.18],\n",
    "    \"superhero\": [0.08, 0.05, 0.11, 0.06, 0.04, 0.07, 0.05],\n",
    "    \"religion\": [0.50, 0.41, 0.20, 0.35, 0.10, 0.40, 0.23],\n",
    "    \"body\": [0.86, 0.62, 0.72, 1.38, 0.68, 1.03, 1.15],\n",
    "    \"eating\": [1.27, 0.60, 0.70, 0.52, 0.76, 0.67, 1.15],\n",
    "    \"sports\": [1.16, 0.64, 0.47, 0.63, 0.46, 0.53, 0.46],\n",
    "    \"death\": [1.05, 0.61, 0.94, 0.35, 0.90, 1.30, 0.65],\n",
    "}\n",
    "\n",
    "# Perform Shapiro-Wilk test for each dataset\n",
    "for label, values in data.items():\n",
    "    stat, p_value = shapiro(values)\n",
    "    print(f\"{label}: p-value = {p_value:.5f}\")\n",
    "    if p_value > 0.05:\n",
    "        print(\"  The data is normally distributed (p > 0.05)\\n\")\n",
    "    else:\n",
    "        print(\"  The data is NOT normally distributed (p <= 0.05)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfaad8-1da1-483f-8181-d2a7f41ff5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISTRIBUTION TEST ON PERSONALITY DATA ###\n",
    "\n",
    "# Big Five 各个性格特征的评分数据\n",
    "neuroticism = [3.22, 4.22, 5.52, 3.0, 3.15, 3.85, 4.43]\n",
    "extroversion = [4.9, 4.65, 4.36, 4.36, 4.23, 3.92, 2.65]\n",
    "openness = [4.02, 5.12, 5.52, 4.27, 3.86, 5.13, 4.08]\n",
    "agreeableness = [3.76, 4.07, 5.07, 1.95, 2.15, 4.11, 2.6]\n",
    "conscientiousness = [3.01, 6.22, 5.73, 4.88, 4.16, 4.36, 5.49]\n",
    "\n",
    "# 将各维度的结果存储在字典中，方便批量检验\n",
    "big_five_data = {\n",
    "    \"Neuroticism\": neuroticism,\n",
    "    \"Extroversion\": extroversion,\n",
    "    \"Openness\": openness,\n",
    "    \"Agreeableness\": agreeableness,\n",
    "    \"Conscientiousness\": conscientiousness\n",
    "}\n",
    "\n",
    "# 对每个性格特征进行 Shapiro-Wilk 检验并打印 p-value 和判断结果\n",
    "for trait, scores in big_five_data.items():\n",
    "    stat, p_value = shapiro(scores)\n",
    "    print(f\"{trait}: p-value = {p_value:.3f}\")\n",
    "    if p_value > 0.05:\n",
    "        print(f\"  The data is normally distributed (p > 0.05)\\n\")\n",
    "    else:\n",
    "        print(f\"  The data is NOT normally distributed (p <= 0.05)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b2a958-7cf8-4f7a-b587-1eda7f60e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRELATION TEST ###\n",
    "\n",
    "# Language feature data\n",
    "data = {\n",
    "    \"Past Tense Verbs\": [4.09, 4.65, 4.50, 4.32, 4.02, 3.63, 3.69],\n",
    "    \"Present Tense Verbs\": [7.36, 8.78, 9.18, 8.98, 9.21, 7.49, 6.88],\n",
    "    \"Future Tense Verbs\": [1.15, 0.82, 0.80, 0.92, 0.52, 1.13, 1.43],\n",
    "    \"Total number words\": [1.00, 0.68, 0.59, 0.69, 0.78, 0.73, 0.65],\n",
    "    \"Total prepositions\": [6.57, 4.67, 5.06, 5.18, 4.86, 7.22, 5.12],  # Not normal\n",
    "    \"Total articles\": [6.03, 3.96, 4.29, 4.72, 4.12, 5.82, 3.88],\n",
    "    \"Total negations\": [1.29, 1.92, 2.00, 2.24, 1.92, 1.26, 1.71],\n",
    "    \"First Person Pronouns\": [4.25, 5.28, 3.70, 5.18, 4.39, 3.39, 6.51],\n",
    "    \"Second Person Pronouns\": [3.11, 2.49, 2.57, 3.74, 2.47, 3.96, 3.83],\n",
    "    \"Third Person Pronouns\": [1.54, 1.84, 1.62, 1.21, 1.97, 1.36, 1.71],\n",
    "    \"Total empathy-related words\": [56.49, 54.39, 52.42, 62.08, 53.47, 55.31, 63.30],\n",
    "    \"money\": [0.57, 0.37, 0.45, 0.17, 0.29, 0.40, 0.51],\n",
    "    \"work\": [10.97, 15.93, 14.09, 13.87, 15.47, 11.18, 12.56],\n",
    "    \"sleep\": [0.50, 0.41, 0.42, 0.23, 0.49, 0.53, 0.28],\n",
    "    \"occupation\": [0.08, 0.04, 0.11, 0.00, 0.07, 0.13, 0.09],\n",
    "    \"family\": [0.96, 0.63, 0.64, 1.44, 0.74, 0.73, 1.11],\n",
    "    \"swearing_terms\": [0.32, 0.30, 0.34, 0.52, 0.64, 0.37, 0.14],\n",
    "    \"leisure\": [0.29, 0.12, 0.22, 0.29, 0.18, 0.27, 0.14],\n",
    "    \"school\": [0.80, 0.23, 0.47, 0.63, 0.18, 0.70, 0.18],\n",
    "    \"optimism\": [0.48, 0.25, 0.47, 0.52, 0.32, 0.53, 0.55],\n",
    "    \"home\": [0.65, 0.75, 0.56, 1.27, 0.78, 0.77, 1.20],\n",
    "    \"sexuality\": [0.10, 0.05, 0.13, 0.00, 0.06, 0.07, 0.18],\n",
    "    \"superhero\": [0.08, 0.05, 0.11, 0.06, 0.04, 0.07, 0.05],\n",
    "    \"religion\": [0.50, 0.41, 0.20, 0.35, 0.10, 0.40, 0.23],\n",
    "    \"body\": [0.86, 0.62, 0.72, 1.38, 0.68, 1.03, 1.15],\n",
    "    \"eating\": [1.27, 0.60, 0.70, 0.52, 0.76, 0.67, 1.15],\n",
    "    \"sports\": [1.16, 0.64, 0.47, 0.63, 0.46, 0.53, 0.46],  # Not normal\n",
    "    \"death\": [1.05, 0.61, 0.94, 0.35, 0.90, 1.30, 0.65],\n",
    "}\n",
    "\n",
    "# Big Five personality scores\n",
    "big_five_data = {\n",
    "    \"Neuroticism\": [3.22, 4.22, 5.52, 3.0, 3.15, 3.85, 4.43],\n",
    "    \"Extroversion\": [4.9, 4.65, 4.36, 4.36, 4.23, 3.92, 2.65],\n",
    "    \"Openness\": [4.02, 5.12, 5.52, 4.27, 3.86, 5.13, 4.08],\n",
    "    \"Agreeableness\": [3.76, 4.07, 5.07, 1.95, 2.15, 4.11, 2.6],\n",
    "    \"Conscientiousness\": [3.01, 6.22, 5.73, 4.88, 4.16, 4.36, 5.49],\n",
    "}\n",
    "\n",
    "# Correlation results storage\n",
    "correlation_results = {}\n",
    "\n",
    "# Perform correlation tests\n",
    "for trait, scores in big_five_data.items():\n",
    "    correlation_results[trait] = {}\n",
    "    for label, values in data.items():\n",
    "        # Use Spearman for non-normal distributions, otherwise use Pearson\n",
    "        if label in [\"Total prepositions\", \"sports\"]:\n",
    "            corr, p_value = spearmanr(scores, values)\n",
    "            test_type = \"Spearman\"\n",
    "        else:\n",
    "            corr, p_value = pearsonr(scores, values)\n",
    "            test_type = \"Pearson\"\n",
    "        \n",
    "        # Store results in dictionary\n",
    "        correlation_results[trait][label] = (test_type, corr, p_value)\n",
    "\n",
    "# Print correlation results\n",
    "for trait, results in correlation_results.items():\n",
    "    print(f\"\\n{trait} Correlations:\")\n",
    "    for label, (test_type, corr, p_value) in results.items():\n",
    "        print(f\"  {label}: {test_type} correlation = {corr:.3f}, p-value = {p_value:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a73f8-31f9-4eb2-8d53-aa0352b91e24",
   "metadata": {},
   "source": [
    "# Results on Correlation Between Language Use and Personality\n",
    "\n",
    "| Lexicon Label                     | Neuroticism | Extroversion | Openness | Agreeableness | Conscientiousness |\n",
    "|-----------------------------------|-------------|--------------|---------|----------------|-------------------|\n",
    "| Past Tense Verbs                  | 0.60782     | 0.11027      | 0.37594 | 0.51510        | 0.30459           |\n",
    "| Present Tense Verbs               | 0.93158     | 0.23155      | 0.57914 | 0.97234        | 0.49114           |\n",
    "| Future Tense Verbs                | 0.81733     | 0.17556      | 0.79429 | 0.95029        | 0.85915           |\n",
    "| Total number words                | -0.658      | 0.453        | -0.554  | -0.119         | **0.00695**       |\n",
    "| Total prepositions                | -0.250      | -0.144       | 0.071   | 0.036          | -0.571            |\n",
    "| Total articles                    | -0.392      | 0.377        | -0.026  | 0.232          | **0.04242**       |\n",
    "| Total negations                   | 0.104       | 0.050        | 0.042   | -0.338         | 0.586              |\n",
    "| First Person Pronouns             | -0.052      | -0.553       | -0.452  | -0.552         | 0.395              |\n",
    "| Second Person Pronouns            | -0.228      | -0.550       | -0.174  | -0.298         | -0.185            |\n",
    "| Third Person Pronouns             | 0.206       | -0.072       | -0.142  | 0.010          | 0.218              |\n",
    "| Total empathy-related words       | -0.279      | -0.585       | -0.504  | -0.631         | 0.027              |\n",
    "| money                             | 0.418       | -0.167       | 0.057   | 0.537          | -0.193            |\n",
    "| work                              | 0.120       | 0.189        | 0.121   | -0.161         | 0.600              |\n",
    "| sleep                             | -0.029      | 0.410        | 0.193   | 0.499          | -0.480            |\n",
    "| occupation                        | 0.505       | -0.302       | 0.350   | 0.586          | -0.149            |\n",
    "| family                            | -0.494      | -0.244       | -0.567  | -0.702         | -0.187            |\n",
    "| swearing_terms                   | -0.571      | 0.451        | -0.254  | -0.429         | -0.339            |\n",
    "| leisure                           | -0.411      | 0.394        | -0.067  | 0.012          | -0.672            |\n",
    "| school                            | -0.277      | 0.453        | 0.110   | 0.263          | -0.600            |\n",
    "| optimism                          | 0.051       | -0.470       | -0.088  | -0.065         | -0.259            |\n",
    "| home                              | -0.342      | -0.574       | -0.473  | **-0.777**     | 0.168              |\n",
    "| sexuality                         | 0.627       | -0.606       | 0.015   | 0.305          | 0.122              |\n",
    "| superhero                         | 0.570       | 0.303        | 0.588   | 0.751          | -0.022            |\n",
    "| religion                          | -0.266      | 0.422        | 0.130   | 0.289          | -0.281            |\n",
    "| body                              | -0.330      | -0.432       | -0.306  | -0.526         | -0.111            |\n",
    "| eating                            | -0.047      | -0.260       | -0.522  | -0.024         | -0.491            |\n",
    "| sports                            | -0.252      | **0.836**    | 0.108   | 0.126          | -0.090            |\n",
    "| death                             | 0.129       | 0.105        | 0.245   | 0.544          | -0.492            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7b020-6d9b-4d29-b0cb-4cd60efb8493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
